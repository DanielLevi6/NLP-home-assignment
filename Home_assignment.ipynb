{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hb1Kyd94F9Xt",
    "outputId": "7b1c0c12-9c4e-4075-a502-7bb01059d1e9"
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import torch\n",
    "import nltk\n",
    "import re\n",
    "import wget\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "nltk.download('punkt', quiet=True) # this module is used to tokenize the text\n",
    "\n",
    "SEED = 1234\n",
    "random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "hHl3jP8BNA_r"
   },
   "outputs": [],
   "source": [
    "# Some utilities to manipulate the corpus\n",
    "\n",
    "def preprocess(text):\n",
    "    \"\"\"Strips #comments and empty lines from a string\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    for line in text.split(\"\\n\"):\n",
    "        line = line.strip()\n",
    "        line = re.sub('#.*$', '', line)\n",
    "        if line != '':\n",
    "            result.append(line)\n",
    "    return result\n",
    "\n",
    "def nltk_normpunc_tokenize(str):\n",
    "    return nltk.tokenize.word_tokenize(str.lower())\n",
    "\n",
    "def split(list, portions, offset):\n",
    "    return ([list[i] for i in range(0, len(list)) if i%portions != offset],\n",
    "          [list[i] for i in range(0, len(list)) if i%portions == offset])\n",
    "\n",
    "def SMSSpamCollection_tokenize(lines):\n",
    "    result = []\n",
    "    for line in lines:\n",
    "        # tokenize\n",
    "        tokens = nltk_normpunc_tokenize(line)\n",
    "        if tokens[0] == \"ham\":\n",
    "            tokens[0] = \"HAM:\"\n",
    "        elif tokens[0] == \"spam\":\n",
    "            tokens[0] = \"SPAM:\"\n",
    "        else:\n",
    "            raise ValueError(\"format problem - uncategorized SMS\")\n",
    "        # add a start of message token\n",
    "        result += [\"<s>\"] + tokens\n",
    "\n",
    "    return result\n",
    "\n",
    "def postprocess(tokens):\n",
    "    return ' '.join(tokens)\\\n",
    "                .replace(\"<s> \", \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MMSplbxbD2Sm"
   },
   "source": [
    "Download the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1 / unknown"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m corpus_filename \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://github.com/DanielLevi6/NLP-home-assignment/tree/main/data/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      2\u001b[0m                   \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSMSSpamCollection\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m----> 4\u001b[0m \u001b[43mwget\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcorpus_filename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Technion/Git/NLP/NLP_venv/lib/python3.10/site-packages/wget.py:533\u001b[0m, in \u001b[0;36mdownload\u001b[0;34m(url, out, bar)\u001b[0m\n\u001b[1;32m    531\u001b[0m \u001b[38;5;66;03m# add numeric ' (x)' suffix if filename already exists\u001b[39;00m\n\u001b[1;32m    532\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(filename):\n\u001b[0;32m--> 533\u001b[0m     filename \u001b[38;5;241m=\u001b[39m \u001b[43mfilename_fix_existing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    534\u001b[0m shutil\u001b[38;5;241m.\u001b[39mmove(tmpfile, filename)\n\u001b[1;32m    536\u001b[0m \u001b[38;5;66;03m#print headers\u001b[39;00m\n",
      "File \u001b[0;32m~/Technion/Git/NLP/NLP_venv/lib/python3.10/site-packages/wget.py:269\u001b[0m, in \u001b[0;36mfilename_fix_existing\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Expands name portion of filename with numeric ' (x)' suffix to\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;124;03mreturn filename that doesn't exist already.\u001b[39;00m\n\u001b[1;32m    267\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    268\u001b[0m dirname \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mu\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 269\u001b[0m name, ext \u001b[38;5;241m=\u001b[39m filename\u001b[38;5;241m.\u001b[39mrsplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    270\u001b[0m names \u001b[38;5;241m=\u001b[39m [x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(dirname) \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mstartswith(name)]\n\u001b[1;32m    271\u001b[0m names \u001b[38;5;241m=\u001b[39m [x\u001b[38;5;241m.\u001b[39mrsplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m names]\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 1)"
     ]
    }
   ],
   "source": [
    "corpus_filename = (\"https://github.com/DanielLevi6/NLP-home-assignment/tree/main/data/\"\n",
    "                  \"SMSSpamCollection.txt\")\n",
    "os.makedirs('data', exist_ok=True)\n",
    "wget.download(corpus_filename, out=\"data/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wbxCTsLiD7L4"
   },
   "source": [
    "Load the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iiLGSGjHE8Le"
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import gutenberg\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "lines = preprocess(gutenberg.raw('carroll-alice.txt'))\n",
    "train_lines, test_lines = split(lines, 12, 0)\n",
    "\n",
    "train_tokens = tokenize_lines(train_lines)\n",
    "test_tokens = tokenize_lines(test_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EejBl8nfCibe"
   },
   "outputs": [],
   "source": [
    "# Extract vocabulary from dataset\n",
    "vocabulary = list(set(train_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jY8Ujeot0hea"
   },
   "outputs": [],
   "source": [
    "# Creating the n-grams\n",
    "def all_ngrams(vocabulary, n):\n",
    "    return list(itertools.product(vocabulary, repeat=n))\n",
    "\n",
    "def ngrams(tokens, n):\n",
    "    return [tuple(tokens[i:i+n]) for i in range(0, len(tokens)-n+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RCH-gJRkD_za",
    "outputId": "070acedb-0de4-4ab5-fda1-59b176e190a2"
   },
   "outputs": [],
   "source": [
    "print(train_tokens[:6])\n",
    "print(ngrams(train_tokens[:6], 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wilWVIGGERv7"
   },
   "outputs": [],
   "source": [
    "# Counting the ngrams\n",
    "def ngram_counts(vocabulary, tokens, n):\n",
    "    context_dict = defaultdict(lambda: defaultdict(int))\n",
    "    for context in all_ngrams(vocabulary, n-1):\n",
    "        for target in vocabulary:\n",
    "            context_dict[context][target] = 0\n",
    "\n",
    "    for ngram, count in Counter(ngrams(tokens, n)).items():\n",
    "        context_dict[ngram[:-1]][ngram[-1]] = count\n",
    "\n",
    "    return context_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hZu_gMOQFPbr"
   },
   "outputs": [],
   "source": [
    "unigram_counts = ngram_counts(vocabulary, train_tokens, 1)\n",
    "bigram_counts = ngram_counts(vocabulary, train_tokens, 2)\n",
    "trigram_counts = ngram_counts(vocabulary, train_tokens, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "NLP_venv",
   "language": "python",
   "name": "nlp_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
