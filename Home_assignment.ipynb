{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "import torch\n",
        "import nltk\n",
        "import re\n",
        "\n",
        "from collections import defaultdict, Counter\n",
        "\n",
        "nltk.download('punkt', quiet=True) # this module is used to tokenize the text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hb1Kyd94F9Xt",
        "outputId": "7b1c0c12-9c4e-4075-a502-7bb01059d1e9"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Some utilities to manipulate the corpus\n",
        "\n",
        "def preprocess(text):\n",
        "  \"\"\"Strips #comments and empty lines from a string\n",
        "  \"\"\"\n",
        "  result = []\n",
        "  for line in text.split(\"\\n\"):\n",
        "    line = line.strip()\n",
        "    line = re.sub('#.*$', '', line)\n",
        "    if line != '':\n",
        "      result.append(line)\n",
        "  return result\n",
        "\n",
        "def nltk_normpunc_tokenize(str):\n",
        "  return nltk.tokenize.word_tokenize(str.lower())\n",
        "\n",
        "def split(list, portions, offset):\n",
        "  return ([list[i] for i in range(0, len(list)) if i%portions != offset],\n",
        "          [list[i] for i in range(0, len(list)) if i%portions == offset])\n",
        "\n",
        "def tokenize_lines(lines):\n",
        "  result = []\n",
        "  for line in lines:\n",
        "    result += [\"<s>\"] + nltk_normpunc_tokenize(line)\n",
        "\n",
        "  return result"
      ],
      "metadata": {
        "id": "hHl3jP8BNA_r"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download the corpus"
      ],
      "metadata": {
        "id": "MMSplbxbD2Sm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uXICO2oXDbB5",
        "outputId": "cf9bef29-1b41-4a44-8d06-46aa1c84fa94"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/lib/python3.10/runpy.py:126: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour\n",
            "  warn(RuntimeWarning(msg))\n",
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Package gutenberg is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "!python -m nltk.downloader gutenberg"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the corpus"
      ],
      "metadata": {
        "id": "wbxCTsLiD7L4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import gutenberg\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "\n",
        "lines = preprocess(gutenberg.raw('carroll-alice.txt'))\n",
        "train_lines, test_lines = split(lines, 12, 0)\n",
        "\n",
        "train_tokens = tokenize_lines(train_lines)\n",
        "test_tokens = tokenize_lines(test_lines)"
      ],
      "metadata": {
        "id": "iiLGSGjHE8Le"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract vocabulary from dataset\n",
        "vocabulary = list(set(train_tokens))"
      ],
      "metadata": {
        "id": "EejBl8nfCibe"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating the n-grams\n",
        "def all_ngrams(vocabulary, n):\n",
        "  return list(itertools.product(vocabulary, repeat=n))\n",
        "\n",
        "def ngrams(tokens, n):\n",
        "  return [tuple(tokens[i:i+n])\n",
        "          for i in range(0, len(tokens)-n+1)]"
      ],
      "metadata": {
        "id": "jY8Ujeot0hea"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_tokens[:6])\n",
        "print(ngrams(train_tokens[:6], 3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RCH-gJRkD_za",
        "outputId": "070acedb-0de4-4ab5-fda1-59b176e190a2"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['<s>', 'chapter', 'i.', 'down', 'the', 'rabbit-hole']\n",
            "[('<s>', 'chapter', 'i.'), ('chapter', 'i.', 'down'), ('i.', 'down', 'the'), ('down', 'the', 'rabbit-hole')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Counting the ngrams\n",
        "def ngram_counts(vocabulary, tokens, n):\n",
        "  context_dict = defaultdict(lambda: defaultdict(int))\n",
        "  for context in all_ngrams(vocabulary, n-1):\n",
        "    for target in vocabulary:\n",
        "      context_dict[context][target] = 0\n",
        "\n",
        "  for ngram, count in Counter(ngrams(tokens, n)).items():\n",
        "    context_dict[ngram[:-1]][ngram[-1]] = count\n",
        "\n",
        "  return context_dict"
      ],
      "metadata": {
        "id": "wilWVIGGERv7"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unigram_counts = ngram_counts(vocabulary, train_tokens, 1)\n",
        "bigram_counts = ngram_counts(vocabulary, train_tokens, 2)\n",
        "trigram_counts = ngram_counts(vocabulary, train_tokens, 3)"
      ],
      "metadata": {
        "id": "hZu_gMOQFPbr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}