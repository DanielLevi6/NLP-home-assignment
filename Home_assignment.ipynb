{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Home assignment - quiz replacement\n",
    "#### Daniel Levi - 315668129"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 - Implement basic unigram, bigram, and trigram language models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hb1Kyd94F9Xt",
    "outputId": "7b1c0c12-9c4e-4075-a502-7bb01059d1e9"
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import nltk\n",
    "import re\n",
    "import wget\n",
    "from sys import getsizeof\n",
    "import os\n",
    "import random\n",
    "\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "nltk.download('punkt', quiet=True) # this module is used to tokenize the text\n",
    "\n",
    "SEED = 1246\n",
    "random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some utilities to manipulate the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "hHl3jP8BNA_r"
   },
   "outputs": [],
   "source": [
    "# This code section was taken from lab2-1, and modified for my needs\n",
    "\n",
    "def preprocess(text):\n",
    "    \"\"\"Strips #comments and empty lines from a string\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    for line in text.split(\"\\n\"):\n",
    "        line = line.strip()\n",
    "        line = re.sub('#.*$', '', line)\n",
    "        if line != '':\n",
    "            result.append(line)\n",
    "    return result\n",
    "\n",
    "def nltk_normpunc_tokenize(str):\n",
    "    return nltk.tokenize.word_tokenize(str.lower())\n",
    "\n",
    "def split(list, portions, offset):\n",
    "    return ([list[i] for i in range(0, len(list)) if i%portions != offset],\n",
    "          [list[i] for i in range(0, len(list)) if i%portions == offset])\n",
    "\n",
    "def SMSSpamCollection_tokenize(lines):\n",
    "    result = []\n",
    "    for line in lines:\n",
    "        # tokenize\n",
    "        tokens = nltk_normpunc_tokenize(line)\n",
    "        if tokens[0] == \"ham\":\n",
    "            tokens[0] = \"HAM\"\n",
    "        elif tokens[0] == \"spam\":\n",
    "            tokens[0] = \"SPAM\"\n",
    "        # add a start of message token\n",
    "        result += [\"<s>\"] + (tokens[:10] if len(tokens) >= 10 else tokens) + [\"</s>\"] # The truncation of the sentences(as written in the pdf report)\n",
    "\n",
    "    return result\n",
    "\n",
    "def postprocess(tokens):\n",
    "    return ' '.join(tokens)\\\n",
    "                .replace(\"<s> \", \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MMSplbxbD2Sm"
   },
   "source": [
    "Download the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/SMSSpamCollection (1).txt'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_filename = (\"https://raw.githubusercontent.com/DanielLevi6/NLP-home-assignment/main/data/SMSSpamCollection.txt\")\n",
    "os.makedirs('data', exist_ok=True)\n",
    "wget.download(corpus_filename, out=\"data/SMSSpamCollection.txt\", bar=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wbxCTsLiD7L4"
   },
   "source": [
    "Load the corpus, read and tokenize it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "iiLGSGjHE8Le"
   },
   "outputs": [],
   "source": [
    "with open(\"data/SMSSpamCollection.txt\", 'r') as fin:\n",
    "    lines = random.choices(preprocess(fin.read()), k=100)\n",
    "    train_lines, test_lines = split(lines, 10, 0)\n",
    "    train_tokens = SMSSpamCollection_tokenize(train_lines)\n",
    "    test_tokens = SMSSpamCollection_tokenize(test_lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract vocabulary from dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "EejBl8nfCibe"
   },
   "outputs": [],
   "source": [
    "vocabulary = list(set(train_tokens)) + list(set(test_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating the n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "jY8Ujeot0hea"
   },
   "outputs": [],
   "source": [
    "# This code section was taken from lab2-1\n",
    "\n",
    "# All possible n-grams\n",
    "def all_ngrams(vocabulary, n):\n",
    "    return list(itertools.product(vocabulary, repeat=n))\n",
    "\n",
    "# The n-grams which appear in the text\n",
    "def ngrams(tokens, n):\n",
    "    return [tuple(tokens[i:i+n]) for i in range(0, len(tokens)-n+1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counting n-grams\n",
    "Creates a 2-D dictionary of all the counts of the possible context+target token\\\n",
    "    - index : context, target token\\\n",
    "    - value : count of the context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "wilWVIGGERv7"
   },
   "outputs": [],
   "source": [
    "# This code section was taken from lab2-1\n",
    "\n",
    "def ngram_counts(vocabulary, tokens, n):\n",
    "    context_dict = defaultdict(lambda: defaultdict(float))\n",
    "    for context in all_ngrams(vocabulary, n-1):\n",
    "        for target in vocabulary:\n",
    "            context_dict[context][target] = 0.0\n",
    "\n",
    "    for ngram, count in Counter(ngrams(tokens, n)).items():\n",
    "        context_dict[ngram[:-1]][ngram[-1]] = count\n",
    "\n",
    "    return context_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creates the count dictionary for each of the model I want to implement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "hZu_gMOQFPbr"
   },
   "outputs": [],
   "source": [
    "unigram_counts = ngram_counts(vocabulary, train_tokens, 1)\n",
    "bigram_counts = ngram_counts(vocabulary, train_tokens, 2)\n",
    "trigram_counts = ngram_counts(vocabulary, train_tokens, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to count the size of each dictionary and get perspective about the size of our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens:   1022\n",
      "Unigrams:    424\n",
      "Bigrams: 179776\n",
      "Trigrams: 76225024\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# This code section was taken from lab2-1\n",
    "\n",
    "tokens_count = len(train_tokens)\n",
    "unigram_count = sum(len(unigram_counts[cntxt]) for cntxt in unigram_counts)\n",
    "bigram_count = sum(len(bigram_counts[cntxt]) for cntxt in bigram_counts)\n",
    "trigram_count = sum(len(trigram_counts[cntxt]) for cntxt in trigram_counts)\n",
    "\n",
    "# Report on the totals\n",
    "print(f\"Tokens: {tokens_count:6}\\n\"\n",
    "     f\"Unigrams: {unigram_count:6}\\n\"\n",
    "     f\"Bigrams: {bigram_count:6}\\n\"\n",
    "     f\"Trigrams: {trigram_count:6}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The n-gram model implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngram_model(ngram_counts):\n",
    "    prob_dict = defaultdict(lambda: defaultdict(float))\n",
    "    for context in ngram_counts.keys():\n",
    "        sigma = sum(ngram_counts[context].values())\n",
    "        for target in ngram_counts[context].keys():\n",
    "            if sigma == 0:\n",
    "                prob_dict[context][target] = 0.0\n",
    "            else:\n",
    "                prob_dict[context][target] = (ngram_counts[context][target] / sigma)\n",
    "            \n",
    "    return prob_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating the actual desired models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_model = ngram_model(unigram_counts)\n",
    "bigram_model = ngram_model(bigram_counts)\n",
    "trigram_model = ngram_model(trigram_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell was taken from lab2-1, for checking the size of the models that we created. That is crucial for our experiment to think about this issue(issue that forced me to reduce the amount of data I used for these models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens:   8440\n",
      "Unigrams:    240\n",
      "Bigrams:  18528\n",
      "Trigrams: 10485864\n"
     ]
    }
   ],
   "source": [
    "print(f\"Tokens: {getsizeof(train_tokens):6}\\n\"\n",
    "      f\"Unigrams: {getsizeof(unigram_model):6}\\n\"\n",
    "      f\"Bigrams: {getsizeof(bigram_model):6}\\n\"\n",
    "      f\"Trigrams: {getsizeof(trigram_model):6}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the typical words for spam messages, like-fantasy, free, urgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "congrats\n",
      "hey\n",
      "sunshine\n",
      "please\n",
      "urgent\n",
      "u\n",
      "someone\n",
      "ur\n",
      "sorry\n",
      "free\n",
      "last\n",
      "fantasy\n"
     ]
    }
   ],
   "source": [
    "for target, prob in trigram_model[(\"<s>\", \"SPAM\")].items():\n",
    "    if prob > 0:\n",
    "        print(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code was taken from lab2-1 for testing the models\n",
    "\n",
    "def sample(model, context):\n",
    "    distrib = model[context]\n",
    "    prob_remaining = random.random()\n",
    "    for token, prob in sorted(distrib.items()):\n",
    "        if prob_remaining < prob:\n",
    "            return token\n",
    "        else:\n",
    "            prob_remaining -= prob\n",
    "    return ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generates 10 tokens context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_sequence(model, start_context, count=10):\n",
    "    random.seed(SEED)\n",
    "    whole_context = list(start_context)\n",
    "    for index in range(count):\n",
    "        next_sample = sample(model, tuple(whole_context[index:]))\n",
    "        whole_context.append(next_sample)\n",
    "        \n",
    "    return whole_context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will test the 3 models for evaluating the context generating process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['</s>', 'go', 'out', 'HAM', 'the', 'in', 'how', 'shall', '<s>', 'dun']\n",
      "['<s>', 'HAM', 'no', 'money', '4', 'walsall', 'tue', '6', 'th', 'march', '</s>']\n",
      "['<s>', 'SPAM', 'fantasy', 'football', 'is', 'back', 'on', 'your', 'tv', '.', 'go', '</s>']\n"
     ]
    }
   ],
   "source": [
    "print(sample_sequence(unigram_model, ()))\n",
    "print(sample_sequence(bigram_model, (\"<s>\",)))\n",
    "print(sample_sequence(trigram_model, (\"<s>\", \"SPAM\",)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perplexity calculation\n",
    "Just as we did in lab2-1, I will implement the neglogprob calculation, for simplifying the perplexity calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test perplexity - unigram: inf\n",
      "Test perplexity - bigram: inf\n",
      "Test perplexity - trigram: inf\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def neglogprob(tokens, model, n):\n",
    "    score = 0.0\n",
    "    context = tokens[0:n-1]\n",
    "    for token in tokens[n-1:]:\n",
    "        prob = model[tuple(context)][token]\n",
    "        if(prob != 0):\n",
    "            score += (-math.log2(prob))\n",
    "        else:\n",
    "            score += math.inf\n",
    "        context = (context + [token])[1:]\n",
    "    \n",
    "    return score\n",
    "\n",
    "\n",
    "def perplexity(tokens, model, n):\n",
    "    nlog = neglogprob(tokens, model, n)\n",
    "    N = (len(tokens) -n +1)\n",
    "    return 2**(nlog / N)\n",
    "\n",
    "print(f\"Test perplexity - unigram: {perplexity(test_tokens, unigram_model, 1):.3f}\\n\"\n",
    "      f\"Test perplexity - bigram: {perplexity(test_tokens, bigram_model, 2):.3f}\\n\"\n",
    "      f\"Test perplexity - trigram: {perplexity(test_tokens, trigram_model, 3):.3f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 - smoothing\n",
    "### Add-delta smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delta smoothing\n",
    "\n",
    "def ngram_model_smoothed(ngram_counts, delta=2):\n",
    "    prob_dict = defaultdict(lambda: defaultdict(int))\n",
    "    for context in ngram_counts:\n",
    "        V = len(ngram_counts[context])\n",
    "        sigma = sum(ngram_counts[context].values())\n",
    "        norm_factor = sigma + (delta * V)\n",
    "        for target in ngram_counts[context].keys():\n",
    "            prob_dict[context][target] = (ngram_counts[context][target] + delta / norm_factor)\n",
    "            \n",
    "    return prob_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test delta=1 smoothed perplexity - trigram: 70.476\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trigram_model_smoothed = ngram_model_smoothed(trigram_counts, 1)\n",
    "print(f\"Test delta=1 smoothed perplexity - trigram: {perplexity(test_tokens, trigram_model_smoothed, 3):.3f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test delta=2 smoothed perplexity - trigram: 70.204\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trigram_model_smoothed = ngram_model_smoothed(trigram_counts, 2)\n",
    "print(f\"Test delta=2 smoothed perplexity - trigram: {perplexity(test_tokens, trigram_model_smoothed, 3):.3f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Knesser-Nay smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngram_model_smoothed_kneser_nay(ngram_counts):\n",
    "    prob_dict = defaultdict(lambda: defaultdict(float))\n",
    "    d = 0.75\n",
    "    for context in ngram_counts.keys():\n",
    "        sigma = sum(ngram_counts[context].values())\n",
    "        unique = 0\n",
    "        for target in ngram_counts[context].keys():\n",
    "            unique += 1\n",
    "        lamda = d * (unique / sigma) if sigma > 0 else 0\n",
    "        for target in ngram_counts[context].keys():\n",
    "            P = unique / len(ngram_counts[context])\n",
    "            prob_dict[context][target] = (abs(ngram_counts[context][target] - d) / (sigma + 1)) + (lamda*P)\n",
    "            \n",
    "    return prob_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test kneser_nay smoothed perplexity - trigram: 0.275\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trigram_model_kneser_ney_smoothed = ngram_model_smoothed_kneser_nay(trigram_counts)\n",
    "print(f\"Test kneser_nay smoothed perplexity - trigram: {perplexity(test_tokens, trigram_model_kneser_ney_smoothed, 3):.3f}\\n\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "NLP_venv",
   "language": "python",
   "name": "nlp_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
